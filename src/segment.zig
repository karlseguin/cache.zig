const std = @import("std");
const cache = @import("cache.zig");

const Allocator = std.mem.Allocator;

pub fn Segment(comptime T: type) type {
	const Entry = cache.Entry(T);
	const List = @import("list.zig").List(*Entry);
	const IS_SIZED = comptime std.meta.hasFn(T, "size");

	return struct {
		// the current size.
		size: u32,

		// the maximum size we should allow this segment to grow to
		max_size: u32,

		// the size we should roughly trim to when we've reached max_size
		target_size: u32,

		// items only get promoted on every N gets.
		gets_per_promote: u8,

		// a double linked list with most recently used items at the head
		// has its own internal mutex for thread-safety.
		list: List,

		// mutex for lookup and size
		mutex: std.Thread.RwLock,

		// key => entry
		lookup: std.StringHashMap(*Entry),

		const Self = @This();

		pub fn init(allocator: Allocator, config: anytype) Self{
			return .{
				.size = 0,
				.mutex = .{},
				.max_size = config.max_size,
				.target_size = config.target_size,
				.gets_per_promote = config.gets_per_promote,
				.list = List.init(),
				.lookup = std.StringHashMap(*Entry).init(allocator),
			};
		}

		pub fn deinit(self: *Self) void {
			var list = &self.list;
			var it = self.lookup.iterator();
			while (it.next()) |kv| {
				const entry = kv.value_ptr.*;
				list.remove(entry._node);
				entry.release();
			}
			self.lookup.deinit();
		}

		pub fn contains(self: *Self, key: []const u8) bool {
			self.mutex.lockShared();
			defer self.mutex.unlockShared();
			return self.lookup.contains(key);
		}

		pub fn get(self: *Self, key: []const u8) ?*Entry {
			const entry = self.getInternal(key) orelse return null;

			if (entry.expired()) {
				// release getInternal's borrow
				entry.release();

				self.mutex.lock();
				_ = self.lookup.remove(key);
				self.size -= entry._size;
				self.mutex.unlock();
				self.list.remove(entry._node);

				// and now release the cache's implicit borrow
				entry.release();
				return null;
			}

			if (@rem(entry.hit(), self.gets_per_promote) == 0) {
				self.list.moveToFront(entry._node);
			}

			return entry;
		}

		pub fn getEntry(self: *Self, key: []const u8) ?*Entry {
			const entry = self.getInternal(key) orelse return null;

			if (!entry.expired() and @rem(entry.hit(), self.gets_per_promote) == 0) {
				self.list.moveToFront(entry._node);
			}

			return entry;
		}

		// Used by both get and getEntry. Those two methods differ in their handling
		// of expiration, but they share the following to fetch the entry.
		fn getInternal(self: *Self, key: []const u8) ?*Entry {
			self.mutex.lockShared();
			const optional_entry = self.lookup.get(key);
			const entry = optional_entry orelse {
				self.mutex.unlockShared();
				return null;
			};

			// Even though entry.borrow() increments entry._gc atomically, it has to
			// be called under the mutex. If we move the call to entry.borrow() after
			// releating the mutex, a del or put could slip in, see that _gc == 0
			// and call removedFromCache.
			// (And, we want _gc incremented atomically, because this is a shared
			// read lock and multiple threads could be accessing the entry concurrently)
			entry.borrow();
			self.mutex.unlockShared();

			return entry;
		}

		pub fn put(self: *Self, allocator: Allocator, key: []const u8, value: T, config: cache.PutConfig) !*Entry {
			const entry_size = if (IS_SIZED) T.size(value) else config.size;
			const expires = @as(u32, @intCast(std.time.timestamp())) + config.ttl;

			var lookup = &self.lookup;
			var existing_entry: ?*Entry = null;
			const entry = try allocator.create(Entry);

			var segment_size = blk: {
				// This blocks exist so that we can correctly errdefer. This it a common
				// issue with errdefer: there can be a point before the function returns
				// where you no longer want to rollback.
				errdefer allocator.destroy(entry);

				const node = try allocator.create(List.Node);
				errdefer allocator.destroy(node);

				// Tempting to only dupe this if this is a new entry for this key. But
				// that adds a lot of complexity and might not even work. First, it would
				// require dupe under lock (in the gop.found_existing == false case).
				// Second, there are features that rely on entry.key. Now, duping under
				// lock is probably worth it in exchange for less duping. And we could
				// point entry.key = gop.key_ptr.*, but I'm not sure that would work
				// in all cases. Like, who/when do we free the key? If we free it
				// when the item is removed from the cache, then you'd have some still
				// referenced entries with an undefined key. Because those entries would
				// no longer be in the cache, maybe that's fine, but it's risky and
				// could easily be messed up in future code.
				const owned_key = try allocator.dupe(u8, key);
				errdefer allocator.free(owned_key);

				node.* = List.Node{.value = entry};
				entry.* = Entry.init(allocator, owned_key, value, entry_size, expires);
				entry._node = node;

				{
					self.mutex.lock();
					defer self.mutex.unlock();

					var size = self.size;
					const gop = try lookup.getOrPut(owned_key);

					if (gop.found_existing) {
						existing_entry = gop.value_ptr.*;
						gop.value_ptr.* = entry;
						gop.key_ptr.* = owned_key;
						size = size - existing_entry.?._size + entry_size;
					} else {
						gop.value_ptr.* = entry;
						size = size + entry_size;
					}
					self.size = size;
					break :blk size;
				}
			};

			var list = &self.list;
			if (existing_entry) |existing| {
				list.remove(existing._node);
				existing.release();
			}
			list.insert(entry._node);

			if (segment_size <= self.max_size) {
				// we're still under our max_size
				return entry;
			}

			// we need to free some space, we're going to free until our segment size
			// is under our target_size
			const target_size = self.target_size;

			self.mutex.lock();
			defer self.mutex.unlock();
			// recheck
			segment_size = self.size;
			while (segment_size > target_size) {
				const removed_node = list.removeTail() orelse break;
				const removed_entry = removed_node.value;

				const existed_in_lookup = lookup.remove(removed_entry.key);
				std.debug.assert(existed_in_lookup == true);

				segment_size -= removed_entry._size;
				removed_entry.release();
			}
			// we're still under lock
			self.size = segment_size;

			return entry;
		}

		// TOOD: singleflight
		pub fn fetch(self: *Self, comptime S: type, allocator: Allocator, key: []const u8, loader: *const fn(state: S, key: []const u8) anyerror!?T, state: S, config: cache.PutConfig) !?*Entry {
			if (self.get(key)) |v| {
				return v;
			}
			if (try loader(state, key)) |value| {
				const entry = try self.put(allocator, key, value, config);
				entry.borrow();
				return entry;
			}
			return null;
		}

		pub fn del(self: *Self, key: []const u8) bool {
			self.mutex.lock();
			const existing = self.lookup.fetchRemove(key);
			const map_entry = existing orelse {
				self.mutex.unlock();
				return false;
			};
			const entry = map_entry.value;
			self.size -= entry._size;
			self.mutex.unlock();

			self.list.remove(entry._node);
			entry.release();
			return true;
		}

		// This is an expensive call (even more so since we know this is being called
		// on each segment). We optimize what we can, by first collecting the matching
		// entries under a shared lock. This is nice since the expensive prefix match
		// won't block concurrent gets.
		pub fn delPrefix(self: *Self, allocator: Allocator, prefix: []const u8) !usize {
			var matching = std.ArrayList(*Entry).init(allocator);
			defer matching.deinit();

			self.mutex.lockShared();
			var it = self.lookup.iterator();
			while (it.next()) |map_entry| {
				if (std.mem.startsWith(u8, map_entry.key_ptr.*, prefix)) {
					try matching.append(map_entry.value_ptr.*);
				}
			}
			self.mutex.unlockShared();

			const entries = matching.items;
			if (entries.len == 0) {
				return 0;
			}

			var lookup = &self.lookup;
			self.mutex.lock();
			for (entries) |entry| {
				self.size -= entry._size;
				_ = lookup.remove(entry.key);
			}
			self.mutex.unlock();

			// list and entry have their own thread safety
			var list = &self.list;
			for (entries) |entry| {
				list.remove(entry._node);
				entry.release();
			}

			return entries.len;
		}
	};
}
